<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>On Policy Annotation: Minimal Human Edits Unlock Massive Gains in LLM Agents | Terminal-Agent</title>
<meta name=keywords content><meta name=description content="GITHUB
1. The Need for a Fast System in Terminal-Based SWE Data Collection
In terminal-based SFT data collection for software engineering (SWE) tasks, system responsiveness is a crucial determinant of both data efficiency and data quality. For annotators who are not proficient with command-line interfaces, two fundamental issues—slow typing speed and difficulty recalling commands—create significant friction in the labeling process.
First, the interaction cost of terminal input is inherently high. Unlike graphical interfaces that offer affordances such as buttons, menus, or auto-completion, terminals rely entirely on textual command entry. Each operation requires the annotator to recall and retype precise commands and parameters, often under the risk of syntax errors. When annotators frequently make syntax or command errors due to limited terminal proficiency, the repeated cycles of editing, rerunning, and verifying become time-consuming and mentally exhausting, causing substantial inefficiency and fatigue throughout the labeling process."><meta name=author content="Terminal-Agent Team"><link rel=canonical href=https://terminal-agent.github.io/blog/annotation/><link crossorigin=anonymous href=/assets/css/stylesheet.5535818ba8080a84fafce624766afd4112e9f8489b7a167457c88f8ea1398972.css integrity="sha256-VTWBi6gICoT6/OYkdmr9QRLp+EibehZ0V8iPjqE5iXI=" rel="preload stylesheet" as=style><link rel=icon href=https://terminal-agent.github.io/img/terminal_agent_logo.png><link rel=icon type=image/png sizes=16x16 href=https://terminal-agent.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://terminal-agent.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://terminal-agent.github.io/apple-touch-icon.png><link rel=mask-icon href=https://terminal-agent.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://terminal-agent.github.io/blog/annotation/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BP9G7L138H"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BP9G7L138H")}</script><meta property="og:title" content="On Policy Annotation: Minimal Human Edits Unlock Massive Gains in LLM Agents"><meta property="og:description" content="GITHUB
1. The Need for a Fast System in Terminal-Based SWE Data Collection
In terminal-based SFT data collection for software engineering (SWE) tasks, system responsiveness is a crucial determinant of both data efficiency and data quality. For annotators who are not proficient with command-line interfaces, two fundamental issues—slow typing speed and difficulty recalling commands—create significant friction in the labeling process.
First, the interaction cost of terminal input is inherently high. Unlike graphical interfaces that offer affordances such as buttons, menus, or auto-completion, terminals rely entirely on textual command entry. Each operation requires the annotator to recall and retype precise commands and parameters, often under the risk of syntax errors. When annotators frequently make syntax or command errors due to limited terminal proficiency, the repeated cycles of editing, rerunning, and verifying become time-consuming and mentally exhausting, causing substantial inefficiency and fatigue throughout the labeling process."><meta property="og:type" content="article"><meta property="og:url" content="https://terminal-agent.github.io/blog/annotation/"><meta property="og:image" content="https://terminal-agent.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-12-15T12:00:00+08:00"><meta property="article:modified_time" content="2025-12-15T12:00:00+08:00"><meta property="og:site_name" content="Terminal-Agent"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://terminal-agent.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="On Policy Annotation: Minimal Human Edits Unlock Massive Gains in LLM Agents"><meta name=twitter:description content="GITHUB
1. The Need for a Fast System in Terminal-Based SWE Data Collection
In terminal-based SFT data collection for software engineering (SWE) tasks, system responsiveness is a crucial determinant of both data efficiency and data quality. For annotators who are not proficient with command-line interfaces, two fundamental issues—slow typing speed and difficulty recalling commands—create significant friction in the labeling process.
First, the interaction cost of terminal input is inherently high. Unlike graphical interfaces that offer affordances such as buttons, menus, or auto-completion, terminals rely entirely on textual command entry. Each operation requires the annotator to recall and retype precise commands and parameters, often under the risk of syntax errors. When annotators frequently make syntax or command errors due to limited terminal proficiency, the repeated cycles of editing, rerunning, and verifying become time-consuming and mentally exhausting, causing substantial inefficiency and fatigue throughout the labeling process."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://terminal-agent.github.io/blog/"},{"@type":"ListItem","position":2,"name":"On Policy Annotation: Minimal Human Edits Unlock Massive Gains in LLM Agents","item":"https://terminal-agent.github.io/blog/annotation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"On Policy Annotation: Minimal Human Edits Unlock Massive Gains in LLM Agents","name":"On Policy Annotation: Minimal Human Edits Unlock Massive Gains in LLM Agents","description":"GITHUB 1. The Need for a Fast System in Terminal-Based SWE Data Collection In terminal-based SFT data collection for software engineering (SWE) tasks, system responsiveness is a crucial determinant of both data efficiency and data quality. For annotators who are not proficient with command-line interfaces, two fundamental issues—slow typing speed and difficulty recalling commands—create significant friction in the labeling process.\nFirst, the interaction cost of terminal input is inherently high. Unlike graphical interfaces that offer affordances such as buttons, menus, or auto-completion, terminals rely entirely on textual command entry. Each operation requires the annotator to recall and retype precise commands and parameters, often under the risk of syntax errors. When annotators frequently make syntax or command errors due to limited terminal proficiency, the repeated cycles of editing, rerunning, and verifying become time-consuming and mentally exhausting, causing substantial inefficiency and fatigue throughout the labeling process.\n","keywords":[],"articleBody":"GITHUB 1. The Need for a Fast System in Terminal-Based SWE Data Collection In terminal-based SFT data collection for software engineering (SWE) tasks, system responsiveness is a crucial determinant of both data efficiency and data quality. For annotators who are not proficient with command-line interfaces, two fundamental issues—slow typing speed and difficulty recalling commands—create significant friction in the labeling process.\nFirst, the interaction cost of terminal input is inherently high. Unlike graphical interfaces that offer affordances such as buttons, menus, or auto-completion, terminals rely entirely on textual command entry. Each operation requires the annotator to recall and retype precise commands and parameters, often under the risk of syntax errors. When annotators frequently make syntax or command errors due to limited terminal proficiency, the repeated cycles of editing, rerunning, and verifying become time-consuming and mentally exhausting, causing substantial inefficiency and fatigue throughout the labeling process.\nSecond, human aversion to repetitive manual typing further amplifies the problem. Typing long or complex commands is cognitively taxing and error-prone, especially for users without strong muscle memory or command-line experience. Combined with slow system feedback, this friction discourages annotators from engaging deeply with each task. In practice, many tend to skip difficult samples or submit minimal edits just to complete the assignment, resulting in lower data diversity and weaker supervision signals for downstream model training.\nTogether, these factors raise the competence threshold for participation in terminal-based annotation. Only annotators with sufficient technical proficiency and patience can maintain throughput and consistency, which limits the scalability of data collection and increases training and management costs. Consequently, a fast and responsive system is not merely a convenience but a prerequisite for obtaining high-quality SFT data in SWE settings.\n2. Edit over Write: An LLM-in-the-Loop Workflow for Terminal Based SWE Annotation 1) Problem and the Shift in Paradigm In terminal based annotation for software engineering tasks, many annotators type slowly and do not remember command syntax well. Writing every command from scratch invites errors and repeated retries. Efficiency suffers because each correction requires new typing and renewed recall of flags and parameters.\nWe address this by shifting from a workflow where humans author the full annotation trajectory to a workflow where humans edit the model’s proposal. The model produces the next candidate command. The human makes a minimal correction only where the proposal is wrong. This keeps human effort focused on judgment rather than on transcription.\nExample\nPreviously, to inspect the most recent 50 lines that contain Timeout, the annotator had to compose:\ngrep -n \"Timeout\" /var/log/app.log | tail -n 50 With LLM in the loop, the model might propose:\ngrep -n \"Error\" /var/log/app.log | tail -n 50 The annotator edits only one token: change \"Error\" to \"Timeout\". No full rewrite is needed.\n2) Why this Division of Labor Works Empirically, LLM are usually competent at command syntax. The more common weaknesses are in strategy or step selection. Humans are better at deciding what to do next, such as which file region to inspect first or which tool is more suitable, while the model can handle the exact flags, quoting, and piping.\nThis division of labor assigns the human to decision making and assigns the model to concrete string production. The human does not need to recall details of sed, awk, jq, or intricate pytest flags. The model turns concise human intent into a runnable command.\nExamples\nThe human decides to read lines 100 to 140 of a large file. The model emits:\nsed -n '100,140p' big.txt The human decides to view the JSON field user.id. The model emits:\njq '.user.id' data.json | head -n 20 3) System Desgin: Interaction Protocol (p \u0026 e) Each step begins with a model proposal for the next operation.\nIf the human judges it reasonable, press p to proceed and execute it. If the human judges it needs adjustment, press e to edit only at the error point and apply a minimal change. After a minimal edit, the system triggers completion. Completion exists because modern language models are causal. Given a prefix, the model continues naturally to the next tokens based on ( P(x_{t} | x_{\u003c t}) ). The goal of completion is to reduce human input and memory load. The human points out the key change and the model fills in the routine details, such as flags, pipes, and ordering.\nExamples\nThe model proposes:\npytest tests/test_user.py::test_login -q The human wants to run only slow tests first. Press e and introduce the key change -m slow. Completion can then add commonly used parameters the team prefers.\nThe model proposes:\ncat config.yaml The human wants only lines that contain timeout. Press e and state that intent. Completion yields a concrete command such as:\ngrep -n \"timeout\" config.yaml | head -n 20 4) Additional Benefits Closer to on policy data. Most tokens are produced by the model and only a small portion is edited by humans. The resulting samples are closer to the model’s own generation distribution, which is useful for subsequent alignment.\nSafer real use. In real usage the same edit based protocol applies. Every step is either accepted or minimally edited before execution. This keeps the process controllable, auditable, and reversible, rather than handing full control to an autonomous agent.\nPositive feedback over time. Continued use accumulates edit data. Fine tuning on these edits makes the model better aligned with human preferences. Over time the accept to edit ratio increases and the average edit distance decreases. The workflow becomes easier the more it is used.\nDiscussion An interesting future direction lies in developing human–AI collaborative systems that unify model assistance and data collection. Instead of maintaining purely human-driven or LLM-driven workflows, future systems could adopt a collaborative design in which humans and LLMs jointly perform tasks. During this process, user edits, acceptances, and rejections can be automatically captured as on-policy SFT data, closely reflecting the model’s real deployment environment.\nA successful example of this paradigm is Cursor, which automatically completes code and allows users to decide whether to accept the suggestions. While users adopt Cursor primarily to enhance coding efficiency, the system simultaneously accumulates rich behavioral data, such as acceptance or rejection signals—that can be used to further fine-tune the model. As this collaborative loop continues, both the model’s performance and the user experience improve over time.\nCitation @misc{reptile2025annotation, title={On Policy Annotation: How Minimal Human Edits Unlock Massive Gains in LLM Agents}, author={Du, Cunxiao and Wang, Tianduo and Dou, Longxu and Li, Shenggui and Zhang, Tianjie and Liu, Tianyu and Chen, Xianwei and Lin, Min}, year={2025}, howpublished={\\url{https://terminal-agent.github.io/blog/annotation/}}, note={Blog}, } ","wordCount":"1086","inLanguage":"en","datePublished":"2025-12-15T12:00:00+08:00","dateModified":"2025-12-15T12:00:00+08:00","author":{"@type":"Person","name":"Terminal-Agent Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://terminal-agent.github.io/blog/annotation/"},"publisher":{"@type":"Organization","name":"Terminal-Agent","logo":{"@type":"ImageObject","url":"https://terminal-agent.github.io/img/terminal_agent_logo.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Sailor (Alt + H)"></a><div class=logo-switches></div></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/terminal-agent/reptile title=Github><span>Github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>On Policy Annotation: Minimal Human Edits Unlock Massive Gains in LLM Agents</h1><div class=post-meta><span title='2025-12-15 12:00:00 +0800 +0800'>December 15, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1086 words&nbsp;·&nbsp;Terminal-Agent Team</div></div></div><main class=main><article class=post-single><div class=post-content><a href=https://github.com/terminal-agent/reptile class="btn external" target=_blank>GITHUB</a><h2 id=1-the-need-for-a-fast-system-in-terminal-based-swe-data-collection>1. The Need for a Fast System in Terminal-Based SWE Data Collection<a hidden class=anchor aria-hidden=true href=#1-the-need-for-a-fast-system-in-terminal-based-swe-data-collection>#</a></h2><p>In terminal-based SFT data collection for software engineering (SWE) tasks, system responsiveness is a crucial determinant of both data efficiency and data quality. For annotators who are not proficient with command-line interfaces, two fundamental issues—slow typing speed and difficulty recalling commands—create significant friction in the labeling process.</p><p>First, <strong>the interaction cost of terminal input is inherently high</strong>. Unlike graphical interfaces that offer affordances such as buttons, menus, or auto-completion, <em>terminals rely entirely on textual command entry</em>. Each operation requires the annotator to <strong>recall and retype precise commands and parameters, often under the risk of syntax errors</strong>. When annotators frequently make syntax or command errors due to limited terminal proficiency, the repeated cycles of editing, rerunning, and verifying become time-consuming and mentally exhausting, causing substantial inefficiency and fatigue throughout the labeling process.</p><p>Second, <strong>human aversion to repetitive manual typing further amplifies the problem</strong>. Typing long or complex commands is cognitively taxing and error-prone, especially for users without strong muscle memory or command-line experience. Combined with slow system feedback, this friction discourages annotators from engaging deeply with each task. In practice, many tend to skip difficult samples or submit minimal edits just to complete the assignment, resulting in lower data diversity and weaker supervision signals for downstream model training.</p><p>Together, these factors raise the competence threshold for participation in terminal-based annotation. Only annotators with sufficient technical proficiency and patience can maintain throughput and consistency, which limits the scalability of data collection and increases training and management costs. Consequently, a fast and responsive system is not merely a convenience but a prerequisite for obtaining high-quality SFT data in SWE settings.</p><h2 id=2-edit-over-write-an-llm-in-the-loop-workflow-for-terminal-based-swe-annotation>2. Edit over Write: An LLM-in-the-Loop Workflow for Terminal Based SWE Annotation<a hidden class=anchor aria-hidden=true href=#2-edit-over-write-an-llm-in-the-loop-workflow-for-terminal-based-swe-annotation>#</a></h2><h3 id=1-problem-and-the-shift-in-paradigm>1) Problem and the Shift in Paradigm<a hidden class=anchor aria-hidden=true href=#1-problem-and-the-shift-in-paradigm>#</a></h3><p>In terminal based annotation for software engineering tasks, many annotators type slowly and do not remember command syntax well. Writing every command from scratch invites errors and repeated retries. Efficiency suffers because each correction requires new typing and renewed recall of flags and parameters.</p><p>We address this by shifting from a workflow where humans author the full annotation trajectory to a workflow where humans edit the model’s proposal. The model produces the next candidate command. The human makes a minimal correction only where the proposal is wrong. This keeps human effort focused on judgment rather than on transcription.</p><p><strong>Example</strong></p><p>Previously, to inspect the most recent 50 lines that contain <em>Timeout</em>, the annotator had to compose:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>grep -n <span class=s2>&#34;Timeout&#34;</span> /var/log/app.log <span class=p>|</span> tail -n <span class=m>50</span>
</span></span></code></pre></div><p>With LLM in the loop, the model might propose:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>grep -n <span class=s2>&#34;Error&#34;</span> /var/log/app.log <span class=p>|</span> tail -n <span class=m>50</span>
</span></span></code></pre></div><p>The annotator edits only one token: change <code>"Error"</code> to <code>"Timeout"</code>. No full rewrite is needed.</p><hr><h3 id=2-why-this-division-of-labor-works>2) Why this Division of Labor Works<a hidden class=anchor aria-hidden=true href=#2-why-this-division-of-labor-works>#</a></h3><p>Empirically, LLM are usually competent at command <strong>syntax</strong>. The more common weaknesses are in <strong>strategy</strong> or <strong>step selection</strong>. Humans are better at deciding what to do next, such as which file region to inspect first or which tool is more suitable, while the model can handle the exact flags, quoting, and piping.</p><p>This division of labor assigns the human to decision making and assigns the model to concrete string production. The human does not need to recall details of <code>sed</code>, <code>awk</code>, <code>jq</code>, or intricate <code>pytest</code> flags. The model turns concise human intent into a runnable command.</p><p><strong>Examples</strong></p><ul><li><p>The human decides to read lines 100 to 140 of a large file. The model emits:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sed -n <span class=s1>&#39;100,140p&#39;</span> big.txt
</span></span></code></pre></div></li><li><p>The human decides to view the JSON field <code>user.id</code>. The model emits:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>jq <span class=s1>&#39;.user.id&#39;</span> data.json <span class=p>|</span> head -n <span class=m>20</span>
</span></span></code></pre></div></li></ul><hr><h3 id=3-system-desgin-interaction-protocol-p--e>3) System Desgin: Interaction Protocol (p & e)<a hidden class=anchor aria-hidden=true href=#3-system-desgin-interaction-protocol-p--e>#</a></h3><p>Each step begins with a model proposal for the next operation.</p><ul><li>If the human judges it reasonable, press <strong>p</strong> to proceed and execute it.</li><li>If the human judges it needs adjustment, press <strong>e</strong> to edit only at the error point and apply a minimal change.</li></ul><p>After a minimal edit, the system triggers <strong>completion</strong>. Completion exists because modern language models are causal. Given a prefix, the model continues naturally to the next tokens based on ( P(x_{t} | x_{&lt; t}) ). The goal of completion is to reduce human input and memory load. The human points out the key change and the model fills in the routine details, such as flags, pipes, and ordering.</p><p><strong>Examples</strong></p><ul><li><p>The model proposes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pytest tests/test_user.py::test_login -q
</span></span></code></pre></div><p>The human wants to run only slow tests first. Press <strong>e</strong> and introduce the key change <code>-m slow</code>. Completion can then add commonly used parameters the team prefers.</p></li><li><p>The model proposes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cat config.yaml
</span></span></code></pre></div><p>The human wants only lines that contain <code>timeout</code>. Press <strong>e</strong> and state that intent. Completion yields a concrete command such as:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>grep -n <span class=s2>&#34;timeout&#34;</span> config.yaml <span class=p>|</span> head -n <span class=m>20</span>
</span></span></code></pre></div></li></ul><hr><h3 id=4-additional-benefits>4) Additional Benefits<a hidden class=anchor aria-hidden=true href=#4-additional-benefits>#</a></h3><p><strong>Closer to on policy data.</strong> Most tokens are produced by the model and only a small portion is edited by humans. The resulting samples are closer to the model’s own generation distribution, which is useful for subsequent alignment.</p><p><strong>Safer real use.</strong> In real usage the same edit based protocol applies. Every step is either accepted or minimally edited before execution. This keeps the process controllable, auditable, and reversible, rather than handing full control to an autonomous agent.</p><p><strong>Positive feedback over time.</strong> Continued use accumulates edit data. Fine tuning on these edits makes the model better aligned with human preferences. Over time the accept to edit ratio increases and the average edit distance decreases. The workflow becomes easier the more it is used.</p><hr><h2 id=discussion>Discussion<a hidden class=anchor aria-hidden=true href=#discussion>#</a></h2><p>An interesting future direction lies in developing <strong>human–AI collaborative systems</strong> that unify model assistance and data collection. Instead of maintaining purely human-driven or LLM-driven workflows, future systems could adopt a collaborative design in which <strong>humans and LLMs jointly perform tasks</strong>. During this process, user edits, acceptances, and rejections can be automatically captured as on-policy SFT data, closely reflecting the model’s real deployment environment.</p><p>A successful example of this paradigm is <em>Cursor</em>, which automatically completes code and allows users to decide whether to accept the suggestions. While users adopt Cursor primarily to enhance coding efficiency, the system simultaneously accumulates rich behavioral data, such as acceptance or rejection signals—that can be used to further fine-tune the model. As this collaborative loop continues, both the model’s performance and the user experience improve over time.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><pre tabindex=0><code>@misc{reptile2025annotation,
  title={On Policy Annotation: How Minimal Human Edits Unlock Massive Gains in LLM Agents},
  author={Du, Cunxiao and Wang, Tianduo and Dou, Longxu and Li, Shenggui and Zhang, Tianjie and Liu, Tianyu and Chen, Xianwei and Lin, Min},
  year={2025},
  howpublished={\url{https://terminal-agent.github.io/blog/annotation/}},
  note={Blog},
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://terminal-agent.github.io/>Terminal-Agent</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span>Acknowledgment to
<a href=https://qwenlm.github.io/ rel="noopener noreferrer" target=_blank>Qwen</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>