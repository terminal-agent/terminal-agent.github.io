<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reptile: Terminal-Agent with Human-in-the-loop Learning | Terminal-Agent</title>
<meta name=keywords content><meta name=description content="GITHUB
Introduction
We propose Reptile, a terminal agent that operates under an extended REPL (Read-Execute-Print-Learn Loop) protocol, where human feedback is seamlessly integrated into the agent&rsquo;s execution loop.
Unlike traditional REPL (Read-Execute-Print Loop) environments that focus solely on code evaluation, our REPL protocol emphasizes the iterative cycle of human-agent collaboration, transforming the terminal from a passive command executor into an interactive learning environment.

  

What Makes Reptile Special?
Compared with other CLI agents (e.g., Claude Code and Mini SWE-Agent), Reptile stands out for the following reasons:"><meta name=author content="Terminal-Agent Team"><link rel=canonical href=https://terminal-agent.github.io/blog/workflow/><link crossorigin=anonymous href=/assets/css/stylesheet.5535818ba8080a84fafce624766afd4112e9f8489b7a167457c88f8ea1398972.css integrity="sha256-VTWBi6gICoT6/OYkdmr9QRLp+EibehZ0V8iPjqE5iXI=" rel="preload stylesheet" as=style><link rel=icon href=https://terminal-agent.github.io/img/terminal_agent_logo.png><link rel=icon type=image/png sizes=16x16 href=https://terminal-agent.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://terminal-agent.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://terminal-agent.github.io/apple-touch-icon.png><link rel=mask-icon href=https://terminal-agent.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://terminal-agent.github.io/blog/workflow/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BP9G7L138H"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BP9G7L138H")}</script><meta property="og:title" content="Reptile: Terminal-Agent with Human-in-the-loop Learning"><meta property="og:description" content="GITHUB
Introduction
We propose Reptile, a terminal agent that operates under an extended REPL (Read-Execute-Print-Learn Loop) protocol, where human feedback is seamlessly integrated into the agent&rsquo;s execution loop.
Unlike traditional REPL (Read-Execute-Print Loop) environments that focus solely on code evaluation, our REPL protocol emphasizes the iterative cycle of human-agent collaboration, transforming the terminal from a passive command executor into an interactive learning environment.

  

What Makes Reptile Special?
Compared with other CLI agents (e.g., Claude Code and Mini SWE-Agent), Reptile stands out for the following reasons:"><meta property="og:type" content="article"><meta property="og:url" content="https://terminal-agent.github.io/blog/workflow/"><meta property="og:image" content="https://terminal-agent.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-12-15T12:00:00+08:00"><meta property="article:modified_time" content="2025-12-15T12:00:00+08:00"><meta property="og:site_name" content="Terminal-Agent"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://terminal-agent.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Reptile: Terminal-Agent with Human-in-the-loop Learning"><meta name=twitter:description content="GITHUB
Introduction
We propose Reptile, a terminal agent that operates under an extended REPL (Read-Execute-Print-Learn Loop) protocol, where human feedback is seamlessly integrated into the agent&rsquo;s execution loop.
Unlike traditional REPL (Read-Execute-Print Loop) environments that focus solely on code evaluation, our REPL protocol emphasizes the iterative cycle of human-agent collaboration, transforming the terminal from a passive command executor into an interactive learning environment.

  

What Makes Reptile Special?
Compared with other CLI agents (e.g., Claude Code and Mini SWE-Agent), Reptile stands out for the following reasons:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://terminal-agent.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Reptile: Terminal-Agent with Human-in-the-loop Learning","item":"https://terminal-agent.github.io/blog/workflow/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reptile: Terminal-Agent with Human-in-the-loop Learning","name":"Reptile: Terminal-Agent with Human-in-the-loop Learning","description":"GITHUB Introduction We propose Reptile, a terminal agent that operates under an extended REPL (Read-Execute-Print-Learn Loop) protocol, where human feedback is seamlessly integrated into the agent\u0026rsquo;s execution loop.\nUnlike traditional REPL (Read-Execute-Print Loop) environments that focus solely on code evaluation, our REPL protocol emphasizes the iterative cycle of human-agent collaboration, transforming the terminal from a passive command executor into an interactive learning environment.\nWhat Makes Reptile Special? Compared with other CLI agents (e.g., Claude Code and Mini SWE-Agent), Reptile stands out for the following reasons:\n","keywords":[],"articleBody":"GITHUB Introduction We propose Reptile, a terminal agent that operates under an extended REPL (Read-Execute-Print-Learn Loop) protocol, where human feedback is seamlessly integrated into the agent’s execution loop.\nUnlike traditional REPL (Read-Execute-Print Loop) environments that focus solely on code evaluation, our REPL protocol emphasizes the iterative cycle of human-agent collaboration, transforming the terminal from a passive command executor into an interactive learning environment.\nWhat Makes Reptile Special? Compared with other CLI agents (e.g., Claude Code and Mini SWE-Agent), Reptile stands out for the following reasons:\nTerminal-only beyond Bash-only: Simple and stateful execution, which is more efficient than bash-only (you don’t need to specify the environment in every command). It doesn’t require the complicated MCP protocol—just a naive bash tool under the REPL protocol.\nHuman-in-the-Loop Learning: Users can inspect every step and provide prompt feedback, i.e., give feedback under the USER role or edit the LLM generation under the ASSISTANT role.\nThis blog focus on workflow and benchmarking.\nSee TTY-use blog for technical details on how to make terminal backend work.\nSee on-policy annotation blog for annotation details on SWE tasks.\nOur Insights in Building General Agents Workflow: Build the universal action space for the LLM, reserving specialized workflows only for high-risk operations.\nEvaluation: Focus on learning efficiency on meta-actions like inspecting file in right way, besides end2end benchmark, which makes the optimization more trackable.\nAnnotation: Correct Agent’s behaviour with clever annotation (like use PDB Debugging for coding), which enjoys stateful re-run and on-policy prediction.\nFirst Target Milestone Extrapolating this trend predicts that, in under a decade, we will see AI agents that can independently complete a large fraction of software tasks that currently take humans days or weeks. https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/\nWhy Terminal Agent? Promising testbed for Agent Learning Wide Applicability: Spans everyday tasks to professional workflows (software engineering, DevOps, containerization) through a single universal interface. Native LLM Compatibility: Terminal protocols are inherently understood through pretraining—no prompt engineering needed, unlike heavyweight protocols like MCP. Core Research Challenges: Naturally encompasses long-horizon reasoning, context management, error recovery, and compositional tool use. Native Universal Protocol The Unix terminal has always been the universal text interface between human and machine. We believe it can serve the same role for AI agents.\nAt its core, the terminal is a text-based REPL protocol with half a century of history and refinement:\nInterpreters: bash, python, node, perl, ruby and countless others Debuggers: gdb, pdb, lldb for interactive debugging Development tools: git, make, docker for workflows System utilities: Thousands of battle-tested Unix tools This mature ecosystem means agents can leverage decades of tooling without reinventing the wheel.\nTTY Implementation Details We build our LLM agent with https://github.com/sail-sg/tty-use, it implements the REPL protocol to use terminal interactively. A key challenge we solve is how to detect that the foreground process has finished its current job and waiting for next interaction.\nFor technique details in tty-use, please see our post at https://terminal-agent.github.io/blog/tool/ and https://x.com/mavenlin/status/1977758827366817929.\nHow REPL enables Human-in-the-loop Learning? Data Annotation Tool for SFT Data Collection Human-in-the-loop isn’t just for runtime—it’s also central to our data collection strategy for further model training.\nPlease refer to https://reptile.github.io/blog/annotation/ for more annotation details and cases.\nData Collection Workflow Agent attempts task: Generate trajectory with current policy Human reviews each step: Approve, correct, or provide better alternative Trajectory labeling: Mark successful trajectories for SFT Failure analysis: Annotate why certain actions failed Dataset curation: Filter and balance collected demonstrations Our data source\nAll branches are automatically logged with a checkpointing hook. User approval / disapproval means something. LLM after feedback \u003e LLM before feedback. LLM after edit \u003e LLM before edit. Feedback \u0026 edit are natural to user. The more you use, the more data you generate to make the model more like you. Usage of the data\nSupervised finetuning. Preference optimization. RLHF (use the data to train reward model then RL) Benchmarking We annotate tasks on SWEgym. After training with 200 interactions, this improves Devstral-2505-22B performance:\nTerminal-bench: 11.3% -\u003e 18.9% SWE-Bench-Verified: 18.6 -\u003e 32.8% Looking Forward We are actively working on several exciting directions:\n1. Terminal Gym for RL Training We aim to build an Terminal Gym that provides a structured environment for reinforcement learning. This includes (1) procise reward modeling (2) robust and scalable dockerized envs (3) easy-to-hard task sets.\n2. Advanced Learning Algorithms We are exploring offline RL, imitation learning, and other techniques to improve sample efficiency for extra-long agent trajectories (\u003e30K length) and ultimately reduce the need for human supervision.\nOpen Source \u0026 Community Reptile is open source and we welcome contributions! Whether you’re interested in:\nAdding new benchmarks and evaluation tasks Improving the hook system Contributing training data Building integrations with other tools like training/inference backends Research discussion and resource collaboration Visit our GitHub repository: https://github.com/terminal-agent/reptile\nWe are inspired by excellent community work such as terminal-bench and mini-SWE-agent. We thank the community for their efforts and valuable insights!\nConclusion The terminal has been humanity’s interface to computers for 50 years. With Reptile, it becomes the interface between humans and AI agents. Reptile represents a new paradigm for terminal agents: one that embraces human collaboration rather than trying to eliminate it.\nBy extending the familiar REPL protocol with a learning layer, we create a system that:\nLeverages the mature Unix ecosystem without reinvention Provides transparency and control through human-in-the-loop interaction Scales naturally to complex, multi-step tasks Citation If you find Reptile useful in your research or applications, please cite:\n@misc{reptile2025workflow, title={Reptile: Terminal-Agent with Human-in-the-loop Learning}, author={Dou, Longxu and Li, Shenggui and Du, Cunxiao and Wang, Tianduo and Zhang, Tianjie and Liu, Tianyu and Chen, Xianwei and Tang, Chenxia and Zhao, Yuanheng and Lin, Min}, year={2025}, howpublished={\\url{https://terminal-agent.github.io/blog/workflow/}}, note={Blog} } Fun fact: The name “Reptile” has a dual meaning: it refers to the REPL (Read-Eval-Print-Learning Loop) workflow in terminal interactions, and also pays homage to OpenAI’s Reptile meta-learning algorithm (2018), which pioneered few-shot adaptation. Like its namesake, our Reptile learns to quickly adapt to new tasks—but through human-in-the-loop collaboration rather than pure algorithmic optimization. Both share the same philosophy: learning efficiently from minimal examples to master diverse tasks.\nReference: On First-Order Meta-Learning Algorithms\n","wordCount":"1008","inLanguage":"en","datePublished":"2025-12-15T12:00:00+08:00","dateModified":"2025-12-15T12:00:00+08:00","author":{"@type":"Person","name":"Terminal-Agent Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://terminal-agent.github.io/blog/workflow/"},"publisher":{"@type":"Organization","name":"Terminal-Agent","logo":{"@type":"ImageObject","url":"https://terminal-agent.github.io/img/terminal_agent_logo.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Sailor (Alt + H)"></a><div class=logo-switches></div></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/terminal-agent/reptile title=Github><span>Github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Reptile: Terminal-Agent with Human-in-the-loop Learning</h1><div class=post-meta><span title='2025-12-15 12:00:00 +0800 +0800'>December 15, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1008 words&nbsp;·&nbsp;Terminal-Agent Team</div></div></div><main class=main><article class=post-single><div class=post-content><a href=https://github.com/terminal-agent/reptile class="btn external" target=_blank>GITHUB</a><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>We propose <strong>Reptile</strong>, a terminal agent that operates under an extended <strong>REPL (Read-Execute-Print-Learn Loop)</strong> protocol, where human feedback is seamlessly integrated into the agent&rsquo;s execution loop.</p><p>Unlike traditional REPL (Read-Execute-Print Loop) environments that focus solely on code evaluation, our REPL protocol emphasizes the iterative cycle of human-agent collaboration, transforming the terminal from a passive command executor into an interactive learning environment.</p><figure style="text-align:center;margin:1rem 0"><img src=https://hackmd.io/_uploads/SkGiD2BWWl.png style="width:60%;display:block;margin:0 auto"></figure><h2 id=what-makes-reptile-special>What Makes Reptile Special?<a hidden class=anchor aria-hidden=true href=#what-makes-reptile-special>#</a></h2><p>Compared with other CLI agents (e.g., Claude Code and Mini SWE-Agent), Reptile stands out for the following reasons:</p><ul><li><p><strong>Terminal-only beyond Bash-only</strong>: Simple and stateful execution, which is more efficient than bash-only (you don&rsquo;t need to specify the environment in every command). It doesn&rsquo;t require the complicated MCP protocol—just a naive bash tool under the REPL protocol.</p></li><li><p><strong>Human-in-the-Loop Learning</strong>: Users can inspect every step and provide prompt feedback, i.e., give feedback under the USER role or edit the LLM generation under the ASSISTANT role.</p></li></ul><p>This blog focus on <strong>workflow</strong> and <strong>benchmarking</strong>.</p><blockquote><p>See <a href=https://terminal-agent.github.io/blog/tool/>TTY-use blog</a> for technical details on how to make terminal backend work.</p><p>See <a href=https://terminal-agent.github.io/blog/annotation/>on-policy annotation blog</a> for annotation details on SWE tasks.</p></blockquote><h2 id=our-insights-in-building-general-agents>Our Insights in Building General Agents<a hidden class=anchor aria-hidden=true href=#our-insights-in-building-general-agents>#</a></h2><p><strong>Workflow</strong>: Build the universal action space for the LLM, reserving specialized workflows only for high-risk operations.</p><p><strong>Evaluation</strong>: Focus on learning efficiency on meta-actions like inspecting file in right way, besides end2end benchmark, which makes the optimization more trackable.</p><p><strong>Annotation</strong>: Correct Agent&rsquo;s behaviour with clever annotation (like use PDB Debugging for coding), which enjoys stateful re-run and on-policy prediction.</p><h2 id=first-target-milestone>First Target Milestone<a hidden class=anchor aria-hidden=true href=#first-target-milestone>#</a></h2><figure style="text-align:center;margin:1rem 0"><img src=https://hackmd.io/_uploads/SyKX2MTMWg.png style="width:50%;display:block;margin:0 auto"></figure><blockquote><p>Extrapolating this trend predicts that, in under a decade, we will see AI agents that can independently complete a large fraction of software tasks that currently take humans days or weeks.
<a href=https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/>https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/</a></p></blockquote><h2 id=why-terminal-agent>Why Terminal Agent?<a hidden class=anchor aria-hidden=true href=#why-terminal-agent>#</a></h2><h3 id=promising-testbed-for-agent-learning>Promising testbed for Agent Learning<a hidden class=anchor aria-hidden=true href=#promising-testbed-for-agent-learning>#</a></h3><ul><li><strong>Wide Applicability</strong>: Spans everyday tasks to professional workflows (software engineering, DevOps, containerization) through a single universal interface.</li><li><strong>Native LLM Compatibility</strong>: Terminal protocols are inherently understood through pretraining—no prompt engineering needed, unlike heavyweight protocols like MCP.</li><li><strong>Core Research Challenges</strong>: Naturally encompasses long-horizon reasoning, context management, error recovery, and compositional tool use.</li></ul><h3 id=native-universal-protocol>Native Universal Protocol<a hidden class=anchor aria-hidden=true href=#native-universal-protocol>#</a></h3><p>The Unix terminal has always been the universal text interface between human and machine. We believe it can serve the same role for AI agents.</p><p>At its core, the terminal is a text-based REPL protocol with half a century of history and refinement:</p><ul><li><strong>Interpreters</strong>: <code>bash</code>, <code>python</code>, <code>node</code>, <code>perl</code>, <code>ruby</code> and countless others</li><li><strong>Debuggers</strong>: <code>gdb</code>, <code>pdb</code>, <code>lldb</code> for interactive debugging</li><li><strong>Development tools</strong>: <code>git</code>, <code>make</code>, <code>docker</code> for workflows</li><li><strong>System utilities</strong>: Thousands of battle-tested Unix tools</li></ul><p>This mature ecosystem means agents can leverage decades of tooling without reinventing the wheel.</p><h3 id=tty-implementation-details>TTY Implementation Details<a hidden class=anchor aria-hidden=true href=#tty-implementation-details>#</a></h3><p>We build our LLM agent with <a href=https://github.com/sail-sg/tty-use>https://github.com/sail-sg/tty-use</a>, it implements the REPL protocol to use terminal interactively. A key challenge we solve is how to detect that the foreground process has finished its current job and waiting for next interaction.</p><p>For technique details in tty-use, please see our post at <a href=https://terminal-agent.github.io/blog/tool/>https://terminal-agent.github.io/blog/tool/</a> and <a href=https://x.com/mavenlin/status/1977758827366817929>https://x.com/mavenlin/status/1977758827366817929</a>.</p><h2 id=how-repl-enables-human-in-the-loop-learning>How REPL enables Human-in-the-loop Learning?<a hidden class=anchor aria-hidden=true href=#how-repl-enables-human-in-the-loop-learning>#</a></h2><h3 id=data-annotation-tool-for-sft-data-collection>Data Annotation Tool for SFT Data Collection<a hidden class=anchor aria-hidden=true href=#data-annotation-tool-for-sft-data-collection>#</a></h3><p>Human-in-the-loop isn&rsquo;t just for runtime—it&rsquo;s also central to our data collection strategy for further model training.</p><figure style="text-align:center;margin:1rem 0"><img src=https://hackmd.io/_uploads/r1G2tMpG-g.png style="width:60%;display:block;margin:0 auto"></figure><p>Please refer to <a href=https://reptile.github.io/blog/annotation/>https://reptile.github.io/blog/annotation/</a> for more annotation details and cases.</p><h3 id=data-collection-workflow>Data Collection Workflow<a hidden class=anchor aria-hidden=true href=#data-collection-workflow>#</a></h3><figure style="text-align:center;margin:1rem 0"><img src=https://hackmd.io/_uploads/SyRziFHWWl.png style="width:60%;display:block;margin:0 auto"></figure><ol><li><strong>Agent attempts task</strong>: Generate trajectory with current policy</li><li><strong>Human reviews each step</strong>: Approve, correct, or provide better alternative</li><li><strong>Trajectory labeling</strong>: Mark successful trajectories for SFT</li><li><strong>Failure analysis</strong>: Annotate why certain actions failed</li><li><strong>Dataset curation</strong>: Filter and balance collected demonstrations</li></ol><p>Our data source</p><ul><li>All branches are automatically logged with a checkpointing hook.</li><li>User approval / disapproval means something.</li><li>LLM after feedback > LLM before feedback.</li><li>LLM after edit > LLM before edit.</li><li>Feedback & edit are natural to user.</li><li>The more you use, the more data you generate to make the model more like you.</li></ul><p>Usage of the data</p><ul><li>Supervised finetuning.</li><li>Preference optimization.</li><li>RLHF (use the data to train reward model then RL)</li></ul><h2 id=benchmarking>Benchmarking<a hidden class=anchor aria-hidden=true href=#benchmarking>#</a></h2><p><img loading=lazy src=https://hackmd.io/_uploads/SJF1dfTMbe.png alt=image></p><p>We annotate tasks on <a href=https://huggingface.co/SWE-Gym>SWEgym</a>. After training with 200 interactions, this improves Devstral-2505-22B performance:</p><ul><li><a href=https://www.tbench.ai/>Terminal-bench</a>: 11.3% -> 18.9%</li><li><a href=https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified>SWE-Bench-Verified</a>: 18.6 -> 32.8%</li></ul><h2 id=looking-forward>Looking Forward<a hidden class=anchor aria-hidden=true href=#looking-forward>#</a></h2><p>We are actively working on several exciting directions:</p><h3 id=1-terminal-gym-for-rl-training>1. Terminal Gym for RL Training<a hidden class=anchor aria-hidden=true href=#1-terminal-gym-for-rl-training>#</a></h3><p>We aim to build an <strong>Terminal Gym</strong> that provides a structured environment for reinforcement learning.
This includes (1) procise reward modeling (2) robust and scalable dockerized envs (3) easy-to-hard task sets.</p><h3 id=2-advanced-learning-algorithms>2. Advanced Learning Algorithms<a hidden class=anchor aria-hidden=true href=#2-advanced-learning-algorithms>#</a></h3><p>We are exploring offline RL, imitation learning, and other techniques to improve sample efficiency for extra-long agent trajectories (>30K length) and ultimately reduce the need for human supervision.</p><h2 id=open-source--community>Open Source & Community<a hidden class=anchor aria-hidden=true href=#open-source--community>#</a></h2><p>Reptile is open source and we welcome contributions! Whether you&rsquo;re interested in:</p><ul><li>Adding new benchmarks and evaluation tasks</li><li>Improving the hook system</li><li>Contributing training data</li><li>Building integrations with other tools like training/inference backends</li><li>Research discussion and resource collaboration</li></ul><p>Visit our GitHub repository: <a href=https://github.com/terminal-agent/reptile>https://github.com/terminal-agent/reptile</a></p><p>We are inspired by excellent community work such as <a href=https://www.tbench.ai/>terminal-bench</a> and <a href=https://github.com/SWE-agent/mini-swe-agent>mini-SWE-agent</a>. We thank the community for their efforts and valuable insights!</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>The terminal has been humanity&rsquo;s interface to computers for 50 years. With Reptile, it becomes the interface between humans and AI agents.
Reptile represents a new paradigm for terminal agents: one that embraces human collaboration rather than trying to eliminate it.</p><p>By extending the familiar REPL protocol with a learning layer, we create a system that:</p><ul><li>Leverages the mature Unix ecosystem without reinvention</li><li>Provides transparency and control through human-in-the-loop interaction</li><li>Scales naturally to complex, multi-step tasks</li></ul><hr><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If you find Reptile useful in your research or applications, please cite:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@misc</span><span class=p>{</span><span class=nl>reptile2025workflow</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span><span class=p>=</span><span class=s>{Reptile: Terminal-Agent with Human-in-the-loop Learning}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span><span class=p>=</span><span class=s>{Dou, Longxu and Li, Shenggui and Du, Cunxiao and Wang, Tianduo and Zhang, Tianjie and Liu, Tianyu and Chen, Xianwei and Tang, Chenxia and Zhao, Yuanheng and Lin, Min}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span><span class=p>=</span><span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>howpublished</span><span class=p>=</span><span class=s>{\url{https://terminal-agent.github.io/blog/workflow/}}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>note</span><span class=p>=</span><span class=s>{Blog}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><hr><blockquote><p><strong>Fun fact</strong>: The name &ldquo;Reptile&rdquo; has a dual meaning: it refers to the REPL (Read-Eval-Print-Learning Loop) workflow in terminal interactions, and also pays homage to OpenAI&rsquo;s Reptile meta-learning algorithm (2018), which pioneered few-shot adaptation. Like its namesake, our Reptile learns to quickly adapt to new tasks—but through human-in-the-loop collaboration rather than pure algorithmic optimization. Both share the same philosophy: learning efficiently from minimal examples to master diverse tasks.</p><p><em>Reference: <a href=https://arxiv.org/abs/1803.02999>On First-Order Meta-Learning Algorithms</a></em></p></blockquote></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://terminal-agent.github.io/>Terminal-Agent</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span>Acknowledgment to
<a href=https://qwenlm.github.io/ rel="noopener noreferrer" target=_blank>Qwen</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>