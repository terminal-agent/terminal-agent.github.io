<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Terminal: LLM’s Last Tool | Terminal-Agent</title>
<meta name=keywords content><meta name=description content="GITHUB
TWITTER
NOTION-Version
LLM systems today are gravitating toward structured “tool protocols”. The most prominent of these, MCP, defines tools through JSON schemas that describe how models can interact with a computer. But here’s the quiet irony: models already know. They’ve seen countless examples of people running commands, inspecting logs, and fixing errors — all through a single interface that’s existed for half a century: the terminal.

The burden of new protocols
Modern tool frameworks like MCP describe every action in meticulous JSON. They are explicit, structured and heavy. To use them, an endless catalog of tool descriptions need to be maintained and explained in the system prompt."><meta name=author content="Terminal-Agent Team"><link rel=canonical href=https://terminal-agent.github.io/blog/tool/><link crossorigin=anonymous href=/assets/css/stylesheet.5535818ba8080a84fafce624766afd4112e9f8489b7a167457c88f8ea1398972.css integrity="sha256-VTWBi6gICoT6/OYkdmr9QRLp+EibehZ0V8iPjqE5iXI=" rel="preload stylesheet" as=style><link rel=icon href=https://terminal-agent.github.io/img/terminal_agent_logo.png><link rel=icon type=image/png sizes=16x16 href=https://terminal-agent.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://terminal-agent.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://terminal-agent.github.io/apple-touch-icon.png><link rel=mask-icon href=https://terminal-agent.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://terminal-agent.github.io/blog/tool/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BP9G7L138H"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BP9G7L138H")}</script><meta property="og:title" content="Terminal: LLM’s Last Tool"><meta property="og:description" content="GITHUB
TWITTER
NOTION-Version
LLM systems today are gravitating toward structured “tool protocols”. The most prominent of these, MCP, defines tools through JSON schemas that describe how models can interact with a computer. But here’s the quiet irony: models already know. They’ve seen countless examples of people running commands, inspecting logs, and fixing errors — all through a single interface that’s existed for half a century: the terminal.

The burden of new protocols
Modern tool frameworks like MCP describe every action in meticulous JSON. They are explicit, structured and heavy. To use them, an endless catalog of tool descriptions need to be maintained and explained in the system prompt."><meta property="og:type" content="article"><meta property="og:url" content="https://terminal-agent.github.io/blog/tool/"><meta property="og:image" content="https://terminal-agent.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-10-15T12:00:00+08:00"><meta property="article:modified_time" content="2025-10-15T12:00:00+08:00"><meta property="og:site_name" content="Terminal-Agent"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://terminal-agent.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Terminal: LLM’s Last Tool"><meta name=twitter:description content="GITHUB
TWITTER
NOTION-Version
LLM systems today are gravitating toward structured “tool protocols”. The most prominent of these, MCP, defines tools through JSON schemas that describe how models can interact with a computer. But here’s the quiet irony: models already know. They’ve seen countless examples of people running commands, inspecting logs, and fixing errors — all through a single interface that’s existed for half a century: the terminal.

The burden of new protocols
Modern tool frameworks like MCP describe every action in meticulous JSON. They are explicit, structured and heavy. To use them, an endless catalog of tool descriptions need to be maintained and explained in the system prompt."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://terminal-agent.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Terminal: LLM’s Last Tool","item":"https://terminal-agent.github.io/blog/tool/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Terminal: LLM’s Last Tool","name":"Terminal: LLM’s Last Tool","description":"GITHUB TWITTER NOTION-Version\nLLM systems today are gravitating toward structured “tool protocols”. The most prominent of these, MCP, defines tools through JSON schemas that describe how models can interact with a computer. But here’s the quiet irony: models already know. They’ve seen countless examples of people running commands, inspecting logs, and fixing errors — all through a single interface that’s existed for half a century: the terminal.\nThe burden of new protocols Modern tool frameworks like MCP describe every action in meticulous JSON. They are explicit, structured and heavy. To use them, an endless catalog of tool descriptions need to be maintained and explained in the system prompt.\n","keywords":[],"articleBody":"GITHUB TWITTER NOTION-Version\nLLM systems today are gravitating toward structured “tool protocols”. The most prominent of these, MCP, defines tools through JSON schemas that describe how models can interact with a computer. But here’s the quiet irony: models already know. They’ve seen countless examples of people running commands, inspecting logs, and fixing errors — all through a single interface that’s existed for half a century: the terminal.\nThe burden of new protocols Modern tool frameworks like MCP describe every action in meticulous JSON. They are explicit, structured and heavy. To use them, an endless catalog of tool descriptions need to be maintained and explained in the system prompt.\nIn contrast, a terminal requires no introduction. You don’t need to describe the protocol to use the commands; model just knows how to invoke them from its pretraining data. The model already knows what cat, ls do. It already knows how to navigate, inspect, and manipulate files, and it knows to use--help for showing usage manual.\nWhy invent and teach the model a new protocol when it already knows one?\nTerminal as the universal interface It is easy to confuse terminal with the bash tool that is already integrated in many workflows. But when we talk about terminal in this post, it is in many ways different from the bash tool. A real terminal session has states. You can export a variable, define a function, open another interpreter and stay there as long as you like until you quit it. The workspace is continuously evolved instead of starting from zero each time. Most real-world tasks human do in the terminal depend on this progressive interaction.\nOne simple example is to ask Claude Code / OpenAI Codex to export an environment variable, and in the followup round task it to echo that variable. It is expected to fail because bash tool only performs stateless execution of shell scripts. A true terminal interface is universal, there is no need to separately implement bash tool, python tool, lua tool, node tool and you name it. LLMs should just open the interpreter of these languages in the terminal like the humans do. In our use case, we can even use the pdb debugger without any extra development. It would have taken quite some nontrivial work if you were to support all the pdb interactive subcommands in the MCP way.\nBesides the universality, terminal also handles control sequences, screen redraws, progress bars etc, which makes the output shorter and much more readable than the raw stdout of a subprocess call.\nThe hard problem: detecting the boundary of REPL The universal protocol used in the terminal is the read-eval-print-loop (REPL), which is all we expect from a text based environment.\nThe agent sends an action in text which is read by the terminal; the terminal evaluates the action which changes its own state; and then it print out information of the state change; It is a natural protocol that human is used to. LLM can use it as well, except for one obstacle, there is no clear indicating signal that the terminal has finished printing and waiting the next round of interaction. This information is only known within the foreground process currently running.\nA simple heuristic is to use rules to determine the end of previous interaction. For example, whether the command prompt $, # or \u003e is the last character on the screen. This may work great for unix shells, but not so robust in many cases. For example, gdb has the prompt (gdb), and in user confirmations it could be [Y/n] . Another common method is to check the screen every a few seconds and let LLM make the decision, but in practice this method is neither robust nor token efficient.\nInstead of using heuristics to detect the intent of read of the foreground process, we dig into how a application process interacts with the terminal. Our solution is a combination of two mechanisms\nDetecting non-canonical mode of TTY A terminal switches between canonical and non-canonical modes. In the canonical mode, the kernel buffers the user’s input until the enter key is pressed, this is the default mode of a terminal. In the non-canonical mode however, the foreground process is in charge of the input handling. For example, when we press Arrow-Up in the bash interpreter, it shows the previous command. This is because bash handles every single key stroke the user sends. In the canonical mode, pressing Arrow-Up would result in a ^[[A . Therefore, being in non-canonical mode is a robust sign that the process is waiting for user’s input.\nBy querying this state, we can detect when the terminal is truly done with the previous command and waiting for the next input.\nHooking input calls via LD_PRELOAD In many cases, the process may still use canonical mode for reading user input, especially for old codes. For example, apt install pausing for [Y/n] is in canonical mode. It would slip through our non-canonical mode detection mechanism. Therefore, we introduce another mechanism that is more thorough in detecting the “read” intent of the foreground process.\nUnder the hood, all file readings in Linux goes through a handful of libc functions, read, scanf,getline, fgets to name a few. There’re also asynchronous reading mechanisms that go through select, poll and epoll , but overall the low level mechanism to read from stdin is enumerable. We wrote a small shared library that provides API hooks into these reading mechanisms and provides a robust signal of the program waiting for user input.\nIntegration with existing workflows via MCP We refer to our tool as tty-use and demonstrate its integration with existing workflows via MCP. First, we can install the python package from https://github.com/sail-sg/tty-use, then register our MCP server using:\n# claude code claude mcp add tty-use -- /absolute/path/to/python -m tty_use.mcp_server # openai codex codex mcp add tty-use -- /absolute/path/to/python -m tty_use.mcp_server Now, let’s demonstrate how Claude Code completes a simple debugging task using pdb. The setup is as follows:\nFiles\n/analyze_sales.py /sale_data.cs\nProblem description I have a Python script called analyze_sales.py that’s supposed to process the CSV file sales_data.csv. Can you help me run it, and if there are any errors, debug them by checking the traceback and fixing the issues? Use pdb.\npdb without our tool pdb with our tool This left figure below shows the actions taken by Claude Code agent without our tty-use tool. We can observe two failure modes:\nThe agent cannot establish an interactive debugging session using pdb. Therefore, when it attempts to use pdb , it generates all debugging commands in a single shot, thus defeating the fundamental purpose of pdb—interactive debugging. In the stark contrast, with our MCP tool (right figure above), the agent behaves more naturally like human software engineers to utilize pdb for interactive debugging.\nIntegration with RL environments tty-use can not only be integrated into existing workflows as a tool but also serve as a rich environment for digital intelligent agents to interact, collect experiences, and evolve within. In other words, it provides an expressive setting for training agents via reinforcement learning (RL).\nThe REPL protocol we adopted closely aligns with the OpenAI-Gym-like API env.step(action) -\u003e state , making it easy to integrate with RL frameworks such as https://github.com/axon-rl/gem. We are currently training agents in this general terminal environment and look forward to sharing more results in the future.\nThe Principle That Never Changed The beauty of this approach is that it covers everything without reinventing the wheel. The unix terminal has always been the universal text interface between human and machine. Now it can be the same for models.\n“Write programs to handle text streams, because that is a universal interface.” — Doug McIlroy\nEverything that LLM needs, file editing, searching, emailing, can be just another text processing program that runs inside the terminal.\nCitation @misc{reptile2025tool, title={Terminal: LLM’s Last Tool}, author={Lin, Min and Liu, Zichen}, year={2025}, howpublished={\\url{https://terminal-agent.github.io/blog/tool/}}, note={Blog}, } ","wordCount":"1321","inLanguage":"en","datePublished":"2025-10-15T12:00:00+08:00","dateModified":"2025-10-15T12:00:00+08:00","author":{"@type":"Person","name":"Terminal-Agent Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://terminal-agent.github.io/blog/tool/"},"publisher":{"@type":"Organization","name":"Terminal-Agent","logo":{"@type":"ImageObject","url":"https://terminal-agent.github.io/img/terminal_agent_logo.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Sailor (Alt + H)"></a><div class=logo-switches></div></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/terminal-agent/reptile title=Github><span>Github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Terminal: LLM’s Last Tool</h1><div class=post-meta><span title='2025-10-15 12:00:00 +0800 +0800'>October 15, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1321 words&nbsp;·&nbsp;Terminal-Agent Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/sail-sg/tty-use class="btn external" target=_blank>GITHUB</a>
<a href=https://x.com/mavenlin/status/1977758827366817929 class="btn external" target=_blank>TWITTER</a>
<a href=https://tinyurl.com/vrwcmpks class="btn external" target=_blank>NOTION-Version</a></p><p>LLM systems today are gravitating toward structured <strong>“tool protocols”.</strong> The most prominent of these, <strong>MCP</strong>, defines tools through JSON schemas that describe how models can interact with a computer. But here’s the quiet irony: models already know. They’ve seen countless examples of people running commands, inspecting logs, and fixing errors — all through a single interface that’s existed for half a century: the <strong>terminal</strong>.</p><hr><h3 id=the-burden-of-new-protocols>The burden of new protocols<a hidden class=anchor aria-hidden=true href=#the-burden-of-new-protocols>#</a></h3><p>Modern tool frameworks like MCP describe every action in meticulous JSON. They are explicit, structured and heavy. To use them, an endless catalog of tool descriptions need to be maintained and explained in the system prompt.</p><p>In contrast, a terminal requires no introduction. You don’t need to describe the protocol to use the commands; model just knows how to invoke them from its pretraining data. The model already knows what <code>cat</code>, <code>ls</code> do. It already knows how to navigate, inspect, and manipulate files, and it knows to use<code>--help</code> for showing usage manual.</p><p><strong>Why invent and teach the model a new protocol when it already knows one?</strong></p><hr><h3 id=terminal-as-the-universal-interface><strong>Terminal as the universal interface</strong><a hidden class=anchor aria-hidden=true href=#terminal-as-the-universal-interface>#</a></h3><p>It is easy to confuse terminal with the bash tool that is already integrated in many workflows. But when we talk about terminal in this post, it is in many ways <strong>different from the bash tool</strong>. A real terminal session has states. You can export a variable, define a function, open another interpreter and stay there as long as you like until you quit it. The workspace is continuously evolved instead of starting from zero each time. Most real-world tasks human do in the terminal depend on this progressive interaction.</p><figure style="text-align:center;margin:1rem 0"><img src=cases/pdb-without-tool.webp alt="Claude Code and OpenAI Codex cannot retrieve the environment variable they previously set using the Bash tool" style="width:50%;display:block;margin:0 auto"><figcaption style=text-align:center><strong>One simple example is to ask Claude Code / OpenAI Codex to export an environment variable, and in the followup round task it to echo that variable. It is expected to fail because bash tool only performs stateless execution of shell scripts.</strong></figcaption></figure><p>A true terminal interface is universal, there is no need to separately implement bash tool, python tool, lua tool, node tool and you name it. LLMs should just open the interpreter of these languages in the terminal like the humans do. In our use case, we can even use the <code>pdb</code> debugger without any extra development. It would have taken quite some nontrivial work if you were to support all the <code>pdb</code> interactive subcommands in the MCP way.</p><p>Besides the universality, terminal also handles control sequences, screen redraws, progress bars etc, which makes the output shorter and much more readable than the raw stdout of a subprocess call.</p><h3 id=the-hard-problem-detecting-the-boundary-of-repl>The hard problem: detecting the boundary of REPL<a hidden class=anchor aria-hidden=true href=#the-hard-problem-detecting-the-boundary-of-repl>#</a></h3><p>The universal protocol used in the terminal is the read-eval-print-loop (REPL), which is all we expect from a text based environment.</p><ul><li>The agent sends an <strong>action</strong> in text which is <strong>read</strong> by the terminal;</li><li>the terminal <strong>evaluates</strong> the action which changes its own state;</li><li>and then it <strong>print</strong> out information of the state change;</li></ul><p>It is a natural protocol that human is used to. LLM can use it as well, except for one obstacle, there is no clear indicating signal that the terminal has finished printing and waiting the next round of interaction. This information is only known within the foreground process currently running.</p><p>A simple heuristic is to use rules to determine the end of previous interaction. For example, whether the command prompt <code>$</code>, <code>#</code> or <code>></code> is the last character on the screen. This may work great for unix shells, but not so robust in many cases. For example, <code>gdb</code> has the prompt <code>(gdb)</code>, and in user confirmations it could be <code>[Y/n]</code> . Another common method is to check the screen every a few seconds and let LLM make the decision, but in practice this method is neither robust nor token efficient.</p><p>Instead of using heuristics to detect the intent of read of the foreground process, we dig into how a application process interacts with the terminal. Our solution is a combination of two mechanisms</p><ol><li><strong>Detecting non-canonical mode of TTY</strong></li></ol><p>A terminal switches between <em>canonical</em> and <em>non-canonical</em> modes. In the canonical mode, the kernel buffers the user’s input until the enter key is pressed, this is the default mode of a terminal. In the non-canonical mode however, the foreground process is in charge of the input handling. For example, when we press Arrow-Up in the bash interpreter, it shows the previous command. This is because bash handles every single key stroke the user sends. In the canonical mode, pressing Arrow-Up would result in a <code>^[[A</code> . Therefore, being in non-canonical mode is a robust sign that the process is waiting for user’s input.</p><p>By querying this state, we can detect when the terminal is truly done with the previous command and waiting for the next input.</p><ol start=2><li><strong>Hooking input calls via LD_PRELOAD</strong></li></ol><p>In many cases, the process may still use canonical mode for reading user input, especially for old codes. For example, <code>apt install</code> pausing for <code>[Y/n]</code> is in canonical mode. It would slip through our non-canonical mode detection mechanism. Therefore, we introduce another mechanism that is more thorough in detecting the “read” intent of the foreground process.</p><p>Under the hood, all file readings in Linux goes through a handful of libc functions, <code>read</code>, <code>scanf</code>,<code>getline</code>, <code>fgets</code> to name a few. There’re also asynchronous reading mechanisms that go through <code>select</code>, <code>poll</code> and <code>epoll</code> , but overall the low level mechanism to read from stdin is enumerable. We wrote a small shared library that provides API hooks into these reading mechanisms and provides a robust signal of the program waiting for user input.</p><h3 id=integration-with-existing-workflows-via-mcp>Integration with existing workflows via MCP<a hidden class=anchor aria-hidden=true href=#integration-with-existing-workflows-via-mcp>#</a></h3><p>We refer to our tool as <code>tty-use</code> and demonstrate its integration with existing workflows via MCP. First, we can install the python package from <a href=https://github.com/sail-sg/tty-use>https://github.com/sail-sg/tty-use</a>, then register our MCP server using:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># claude code</span>
</span></span><span class=line><span class=cl>claude mcp add tty-use -- /absolute/path/to/python -m tty_use.mcp_server
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># openai codex</span>
</span></span><span class=line><span class=cl>codex mcp add tty-use -- /absolute/path/to/python -m tty_use.mcp_server
</span></span></code></pre></div><p>Now, let&rsquo;s demonstrate how Claude Code completes a simple debugging task using <code>pdb</code>. The setup is as follows:</p><blockquote><p><strong>Files</strong></p><p><code>/analyze_sales.py</code>
<code>/sale_data.cs</code></p></blockquote><blockquote><p><strong>Problem description</strong>
I have a Python script called <code>analyze_sales.py</code> that&rsquo;s supposed to process the CSV file <code>sales_data.csv</code>. Can you help me run it, and if there are any errors, debug them by checking the traceback and fixing the issues? Use <code>pdb</code>.</p></blockquote><table><tr><td width=50% align=center><a href=cases/pdb-without-tool.webp target=_blank rel="noopener noreferrer"><img src=cases/pdb-without-tool.webp alt="pdb without our tool" style=width:100%;max-width:100%;height:auto;border-radius:4px;cursor:pointer></a><br><strong>pdb without our tool</strong></td><td width=50% align=center><a href=cases/pdb-with-tool.webp target=_blank rel="noopener noreferrer"><img src=cases/pdb-with-tool.webp alt="pdb with our tool" style=width:100%;max-width:100%;height:auto;border-radius:4px;cursor:pointer></a><br><strong>pdb with our tool</strong></td></tr></table><p>This left figure below shows the actions taken by Claude Code agent <em><strong>without</strong></em> our tty-use tool. We can observe two failure modes:</p><ol><li>The agent cannot establish an interactive debugging session using <code>pdb</code>.</li><li>Therefore, when it attempts to use <code>pdb</code> , it generates all debugging commands in a single shot, thus defeating the fundamental purpose of <code>pdb</code>—interactive debugging.</li></ol><p>In the stark contrast, <em><strong>with</strong></em> our MCP tool (right figure above), the agent behaves more naturally like human software engineers to utilize <code>pdb</code> for interactive debugging.</p><h3 id=integration-with-rl-environments>Integration with RL environments<a hidden class=anchor aria-hidden=true href=#integration-with-rl-environments>#</a></h3><p><code>tty-use</code> can not only be integrated into existing workflows as a tool but also serve as a rich environment for digital intelligent agents to interact, collect experiences, and evolve within. In other words, it provides an expressive setting for training agents via reinforcement learning (RL).</p><p>The REPL protocol we adopted closely aligns with the OpenAI-Gym-like API <code>env.step(action) -> state</code> , making it easy to integrate with RL frameworks such as <a href=https://github.com/axon-rl/gem>https://github.com/axon-rl/gem</a>. We are currently training agents in this general terminal environment and look forward to sharing more results in the future.</p><hr><h3 id=the-principle-that-never-changed>The Principle That Never Changed<a hidden class=anchor aria-hidden=true href=#the-principle-that-never-changed>#</a></h3><p>The beauty of this approach is that it covers everything without reinventing the wheel. The unix terminal has always been the universal text interface between human and machine. Now it can be the same for models.</p><blockquote><p>“Write programs to handle text streams, because that is a universal interface.” — <a href=https://en.wikipedia.org/wiki/Doug_McIlroy>Doug McIlroy</a></p></blockquote><p>Everything that LLM needs, file editing, searching, emailing, can be just another text processing program that runs inside the terminal.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><pre tabindex=0><code>@misc{reptile2025tool,
  title={Terminal: LLM’s Last Tool},
  author={Lin, Min and Liu, Zichen},
  year={2025},
  howpublished={\url{https://terminal-agent.github.io/blog/tool/}},
  note={Blog},
}
</code></pre></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://terminal-agent.github.io/>Terminal-Agent</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span>Acknowledgment to
<a href=https://qwenlm.github.io/ rel="noopener noreferrer" target=_blank>Qwen</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>